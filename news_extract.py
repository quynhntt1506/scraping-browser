from lxml.html.clean import clean_html
from lxml import html
import json

# Gi·∫£ s·ª≠ html_content ƒë√£ ƒë∆∞·ª£c l·∫•y t·ª´ m·ªôt trang web
def scraping_data (htmls):
    output_data = {}
    index = 0
    for html_path in htmls:
        index = index + 1
        tree = html.fromstring(html_path)
        # Danh s√°ch ch·ª©a c√°c object con trong sections
        sections = []

        # H√†m t√¨m class g·∫ßn nh·∫•t
        def get_nearest_class(elem):
            while elem is not None:
                class_name = elem.get("class")
                if class_name:  # N·∫øu t√¨m th·∫•y class, tr·∫£ v·ªÅ ngay
                    return class_name
                elem = elem.getparent()  # Di chuy·ªÉn l√™n th·∫ª cha
            return "no-class"  # N·∫øu kh√¥ng c√≥ class n√†o

        for elem in tree.xpath("//*"):
            tag = elem.tag  # Lo·∫°i th·∫ª (p, div, a, img, picture, ...)
            class_name = elem.get("class") or get_nearest_class(elem)  # L·∫•y class g·∫ßn nh·∫•t

            if tag == "a":  # N·∫øu l√† th·∫ª <a>
                href = elem.get("href", "").strip()
                text = (elem.text or "").strip()
                if href and text:
                    sections.append({
                        "class-name": class_name,
                        "type": "link",
                        "text": text,
                        "href": href
                    })

            # elif tag == "img" or tag == 'svg':  # N·∫øu l√† th·∫ª <img>
            #     src = elem.get("src", "").strip()
            #     alt = elem.get("alt", "").strip()
            #     if src:
            #         sections.append({
            #             "class-name": class_name,
            #             "type": "image",
            #             "src": src,
            #             "alt": alt
            #         })

            elif tag == "picture" or tag == "img" or tag == 'svg':  # N·∫øu l√† th·∫ª <picture>
                sources = ""
                alt = ""
                if tag == "picture":
                    img = elem.find(".//img")
                    sources = next((source.get("srcset") for source in elem.xpath(".//source") if source.get("srcset")), None) or img.get("src") if img is not None else ""
                else:
                    sources = elem.get("src", "").strip()
                    alt = elem.get("alt", "").strip()
                    
                # sources = [source.get("srcset") for source in elem.xpath(".//source") if source.get("srcset")]
                # img = elem.find(".//img")
                # img_src = img.get("src") if img is not None else ""
                if sources:
                    sections.append({
                        "class-name": class_name,
                        "type": "image",
                        "sources": sources,
                        "alt": alt
                    })

            else:  # C√°c th·∫ª kh√°c
                text_content = (elem.text or "").strip()
                if text_content:
                    sections.append({
                        "class-name": class_name,
                        "type": "text",
                        "content": text_content
                    })
        # Lo·∫°i b·ªè c√°c object c√≥ class-name tr√πng l·∫∑p
        unique_class_names = set()
        filtered_sections = []

        for obj in sections:
            class_name = obj["class-name"]
            if class_name not in unique_class_names:
                unique_class_names.add(class_name)
                filtered_sections.append(obj)
                
                
        # Ghi v√†o file JSON
        output_data.update({f"sections_{index}": filtered_sections})

    with open("output.json", "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=4)

    print("ƒê√£ l∆∞u d·ªØ li·ªáu v√†o output-one-element.json üéØ")



# def extract_html_data(html_tags):
#     """ Tr√≠ch xu·∫•t th√¥ng tin t·ª´ danh s√°ch th·∫ª HTML v√† hi·ªÉn th·ªã d∆∞·ªõi d·∫°ng b·∫£ng """
#     data = []
    
#     for html in html_tags:
#         soup = BeautifulSoup(html, "html.parser")
#         element = soup.find()  # L·∫•y th·∫ª ƒë·∫ßu ti√™n trong chu·ªói HTML
        
#         if element:
#             tag_name = element.name
#             class_name = " ".join(element.get("class", []))  # L·∫•y class n·∫øu c√≥
#             text = element.get_text(strip=True)  # L·∫•y n·ªôi dung text
            
#             data.append({"tag_name": tag_name, "class_name": class_name, "text": text})
    
#     df = pd.DataFrame(data)
#     return df

# # V√≠ d·ª• 3 th·∫ª HTML ƒë·∫ßu v√†o
# html_tags = [
#     '<p class="highlight-border">Bayer Leverkusen, <a href="https://vietnamnet.vn/bayern-munich-tag2357987485850534144.html" target="_blank"><strong>Bayern Munich</strong></a> cu·ªëi c√πng c≈©ng ƒë√°nh b·∫°i ƒë∆∞·ª£c ƒë·ªôi b√≥ng do Xabi Alonso d·∫´n d·∫Øt ·ªü th·ªùi ƒëi·ªÉm quan tr·ªçng - l∆∞·ª£t ƒëi v√≤ng 16 ƒë·ªôi <a href="https://vietnamnet.vn/champions-league-tag13336189034300601840.html"><strong>Champions League</strong></a>.</p>',
#     '<p class="highlight-border">Ch·ªâ h∆°n 2 tu·∫ßn sau khi may m·∫Øn tho√°t thua Leverkusen t·∫°i BayArena, l·∫ßn n√†y CLB x·ª© Bavaria mang h√¨nh ·∫£nh kh√°c.</p>',
#     '<p class="highlight-border"><em>‚Äúƒê√¢y l√† b∆∞·ªõc ti·∫øn l·ªõn ƒë√∫ng h∆∞·ªõng. T·∫•t c·∫£ m·ªçi ng∆∞·ªùi ƒë·ªÅu bi·∫øt g·∫ßn ƒë√¢y ch√∫ng t√¥i g·∫∑p r·∫•t nhi·ªÅu kh√≥ khƒÉn khi ƒë·ªëi ƒë·∫ßu v·ªõi Leverkusen. H·ªç l√† ƒë·ªôi b√≥ng th·∫≠t s·ª± r·∫•t m·∫°nh‚Äù</em>, Harry Kane chia s·∫ª v·ªÅ chi·∫øn th·∫Øng.</p>'
# ]

# # G·ªçi h√†m v√† in k·∫øt qu·∫£
# df = extract_html_data(html_tags)
# print(df)






